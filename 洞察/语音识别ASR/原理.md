# [学习]语音语言模型发展史-台大李宏毅老师
https://youtu.be/gkAyqoQkOSk
ai好记笔记：https://aihaoji.com/zh/dashboard/tasks/view/article/0cdad8b1-3053-89b7-a376-6151ee01bd10?share_token=C9fRFNeKZ6nWqaD5lP

what：输入audio，输出audio。Conversational Speech Model
![](attachments/Pasted%20image%2020250914233704.png)
挑战：要识别人、环境噪音、情绪等等

现在有哪些模型？目前最惊艳的是sesame，很多模型实际上是ASR-》语音合成
![](attachments/Pasted%20image%2020250914233824.png)
原理：类似于NLP的这种。然后也是经过pretrain->sft->rlhf
![](attachments/Pasted%20image%2020250915000040.png)
CSR的token是什么？
![](attachments/Pasted%20image%2020250915000204.png)
solution1：采用文字。这个其实很不够，因为变成文字之后，忽略了情绪。
![](attachments/Pasted%20image%2020250915000328.png)
solution2：用最小的frame。这样，一秒钟8K取样点。问题在于token太多，性能差。
![](attachments/Pasted%20image%2020250915000437.png)
历史上有很多tokenization的方法
![](attachments/Pasted%20image%2020250915001102.png)
哪一种tokenizer是最好的呢？有一个benchmark
![](attachments/Pasted%20image%2020250915001337.png)
怎么训练出这个tokenizer的呢？

方式1：self-supervised model
1）切成0.02s一个segments，然后用self supervised learning model训练出一个vector。（Next token prediction）
![](attachments/Pasted%20image%2020250915001447.png)
2）连续的向量聚类成离散的，右边这一坨就叫做tokenization。注意最后的这个tokenized的segments，并不一定都相同长度（因为会有同类项合并）。
![](attachments/Pasted%20image%2020250915001630.png)
3）这个tokenization的东西怎么decode成语音呢？同样训练一个decode的模型
![](attachments/Pasted%20image%2020250915001749.png)
方式2：Neural Speech Codec
tokenizer和detokenizer同时训练，类似于auto-encoder。latent discrete representation
![](attachments/Pasted%20image%2020250915001927.png)
总结一下，注意一下这里的semantic token实际上更像是音标，而不是真的semantic的东西。
![](attachments/Pasted%20image%2020250915002417.png)
然后现在的算法，更像是两个的大杂烩 -- 利用不同层级的token。
![](attachments/Pasted%20image%2020250915002734.png)
但是这种multi-scale的方法然streaming。因为它需要利用coarse-grained的token，用于生成Fine_grained token。

然后还有各种parallel的奇技淫巧。
![](attachments/Pasted%20image%2020250915003006.png)
中间省略一堆。有了discrete token之后，再合成最后的speech，采用TTS。TTS可以同时也输入一段语音，这样就利用了原始的人的声音。
![](attachments/Pasted%20image%2020250915003841.png)
后面还有太多的东西了，先使用再回头看吧